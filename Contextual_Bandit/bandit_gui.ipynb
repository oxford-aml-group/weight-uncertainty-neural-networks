{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../BayesCNN/')\n",
    "from utils import DEVICE\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Var = lambda x, dtype=torch.FloatTensor: Variable(\n",
    "    torch.from_numpy(x).type(dtype)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CUDA NOT YET IMPLEMENTED - DISABLE IN BayesBackpropagation.py ###\n",
    "\n",
    "# Import data from file\n",
    "df = pd.read_csv(os.getcwd() + '/Pytorch/agaricus-lepiota.data', sep=',', header=None,\n",
    "             error_bad_lines=False, warn_bad_lines=True, low_memory=False)\n",
    "\n",
    "# Set pandas to output all of the columns in output\n",
    "df.columns = ['class','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "         'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "         'stalk-surf-above-ring','stalk-surf-below-ring','stalk-color-above-ring','stalk-color-below-ring',\n",
    "         'veil-type','veil-color','ring-number','ring-type','spore-color','population','habitat']\n",
    "\n",
    "# Split context from label\n",
    "X = pd.DataFrame(df, columns=df.columns[1:len(df.columns)], index=df.index)\n",
    "# Put the class values (0th column) into Y\n",
    "Y = df['class']\n",
    "\n",
    "# Transform labels into one-hot encoded array\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y)\n",
    "y = le.transform(Y)\n",
    "\n",
    "# Temporary variable to avoid error \n",
    "x_tmp = pd.DataFrame(X,columns=[X.columns[0]])\n",
    "\n",
    "# Encode each feature column and add it to x_train \n",
    "for colname in X.columns:\n",
    "    le.fit(X[colname])\n",
    "    #print(colname, le.classes_)\n",
    "    x_tmp[colname] = le.transform(X[colname])\n",
    "\n",
    "# Produce mushroom array: 8124 mushrooms, each with 117 one-hot encoded features\n",
    "oh = preprocessing.OneHotEncoder(categorical_features='all')\n",
    "oh.fit(x_tmp)\n",
    "x = oh.transform(x_tmp).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(eaten, edible):\n",
    "    # REWARDS FOR AGENT\n",
    "    #  Eat poisonous mushroom\n",
    "    if not eaten:\n",
    "        return 0\n",
    "    if eaten and edible:\n",
    "        return 5\n",
    "    elif eaten and not edible:\n",
    "        return 5 if np.random.rand() > 0.5 else -35\n",
    "\n",
    "def oracle_reward(edible):\n",
    "    return 5*edible    \n",
    "\n",
    "# Define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EAT, REJECT = torch.Tensor([1, 0]).to(DEVICE), torch.Tensor([0, 1]).to(DEVICE)\n",
    "\n",
    "class MushroomNet():\n",
    "    def __init__(self, label = 'MushNet', n_weight_sampling=2):\n",
    "        self.label = label\n",
    "        self.n_weight_sampling = n_weight_sampling\n",
    "        self.epsilon = 0\n",
    "        self.net = None\n",
    "        self.loss, self.optimizer = None, None\n",
    "        self.cum_regrets = [0]\n",
    "        self.bufferX, self.bufferY = [], []\n",
    "    \n",
    "    def init_buffer(self):\n",
    "        for i in np.random.choice(range(len(x)), 4096):\n",
    "            eat = np.random.rand()>0.5\n",
    "            action = [1, 0] if eat else [0, 1]\n",
    "            self.bufferX.append(np.concatenate((x[i], action)))\n",
    "            self.bufferY.append(get_reward(eat, y[i]))\n",
    "            \n",
    "    # Use NN to decide next action\n",
    "    def try_ (self, mushroom):\n",
    "        samples = self.n_weight_sampling\n",
    "        context, edible = x[mushroom], y[mushroom]\n",
    "        try_eat = Var(np.concatenate((context, [1, 0])))\n",
    "        try_reject = Var(np.concatenate((context, [0, 1])))\n",
    "        \n",
    "        # Calculate rewards using model\n",
    "        with torch.no_grad():\n",
    "            r_eat = sum([self.net(try_eat) for _ in range(samples)]).item()\n",
    "            r_reject = sum([self.net(try_reject) for _ in range(samples)]).item()\n",
    "        \n",
    "        # Take random action for epsilon greedy agents, calculate agent's reward\n",
    "        eaten = r_eat > r_reject\n",
    "        if np.random.rand()<self.epsilon:\n",
    "            eaten = (np.random.rand()<.5)\n",
    "        agent_reward = get_reward(eaten, edible)\n",
    "        \n",
    "        # Get rewards and update buffer\n",
    "        action = np.array([1, 0] if eaten else [0, 1])\n",
    "        self.bufferX.append(np.concatenate((context, action)))\n",
    "        self.bufferY.append(agent_reward)\n",
    "        \n",
    "        # Calculate regret\n",
    "        oracle = oracle_reward(edible)\n",
    "        regret = oracle - agent_reward\n",
    "        self.cum_regrets.append(self.cum_regrets[-1]+regret)\n",
    "            \n",
    "\n",
    "    # Feed next mushroom\n",
    "    def update(self, mushroom):\n",
    "        self.try_(mushroom)\n",
    "        # idx pool\n",
    "        l = len(self.bufferX)\n",
    "        idx_pool = range(l) if l >= 4096 else ((int(4096//l) + 1)*\n",
    "                                                       list(range(l)))\n",
    "        idx_pool = np.random.permutation(idx_pool[-4096:])\n",
    "        context_pool = torch.Tensor([self.bufferX[i] for i in idx_pool]).to(DEVICE)\n",
    "        value_pool = torch.Tensor([self.bufferY[i] for i in idx_pool]).to(DEVICE)\n",
    "        for i in range(0, 4096, 64):\n",
    "            self.loss_step(context_pool[i:i+64], value_pool[i:i+64], i//64)\n",
    "        \n",
    "    \n",
    "    def loss_step(self, x, y):\n",
    "        raise NotImplementedError\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for BBB agent\n",
    "from bayes import BayesWrapper\n",
    "from utils import prior_nll\n",
    "\n",
    "def mlp(inputs):\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(inputs, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 100), nn.ReLU(),\n",
    "        nn.Linear(100, 1)).to(DEVICE)\n",
    "    return net\n",
    "\n",
    "class BBB_MNet(MushroomNet):\n",
    "    def __init__(self, label, lr=2e-5):\n",
    "        super().__init__(label)\n",
    "        self.net = BayesWrapper(name='BayesMush',\n",
    "                               net = mlp(x.shape[1]+2),\n",
    "                               prior_nll=prior_nll,\n",
    "                               type='regression',\n",
    "                               lr=lr,\n",
    "                               rho_init=-4)\n",
    "\n",
    "    def loss_step(self, x, y, batch_id, n_samples = 2):\n",
    "        beta = 2 ** (64 - (batch_id + 1)) / (2 ** 64 - 1) \n",
    "        outputs = [self.net(x)[:,0] for _ in range(n_samples)]\n",
    "        self.net.step(outputs, y, beta)\n",
    "\n",
    "\n",
    "# Class for Greedy agents\n",
    "class EpsGreedyMlp(MushroomNet):\n",
    "    def __init__(self, epsilon=0, lr=2e-5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_weight_sampling = 1\n",
    "        self.epsilon = epsilon\n",
    "        self.net = mlp(x.shape[1]+2)\n",
    "        self.optimizer = optim.SGD(self.net.parameters(), lr=lr)\n",
    "        self.mse = lambda x, y:.5*((x-y)**2).sum()\n",
    "        \n",
    "    def loss_step(self, x, y, batch_id):\n",
    "        self.net.zero_grad()\n",
    "        loss = self.mse(self.net.forward(x), y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-5\n",
    "mnets = {'BBB':BBB_MNet(label='bbb', lr=lr),\n",
    "         'Greedy':EpsGreedyMlp(lr=lr, epsilon=0),\n",
    "         'Greedy 1%':EpsGreedyMlp(lr=lr, epsilon=.01),\n",
    "         'Greedy 5%':EpsGreedyMlp(lr=lr, epsilon=.05)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_STEPS = 50000\n",
    "\n",
    "for step in tqdm(range(NB_STEPS)):\n",
    "    mushroom = np.random.randint(len(x))\n",
    "    for name, net in mnets.items():\n",
    "        net.update(mushroom)\n",
    "    if (step+1)%1000 == 0:\n",
    "        df = pd.DataFrame.from_dict({name:net.cum_regrets for \n",
    "                                name,net in mnets.items()})\n",
    "        df.to_csv('Results/mushroom_regrets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
